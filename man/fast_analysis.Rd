% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Fast_analysis.R
\name{fast_analysis}
\alias{fast_analysis}
\title{Efficiently analyze nucleotide recoding data}
\usage{
fast_analysis(
  df,
  pnew = NULL,
  pold = NULL,
  no_ctl = FALSE,
  read_cut = 50,
  features_cut = 10,
  nbin = NULL,
  prior_weight = 2,
  MLE = TRUE,
  lower = -7,
  upper = 7,
  se_max = 2.5,
  mut_reg = 0.1,
  p_mean = 0,
  p_sd = 1,
  StanRate = FALSE,
  Stan_data = NULL
)
}
\arguments{
\item{df}{Dataframe in form provided by cB_to_Fast}

\item{pnew}{Labeled read mutation rate; default of 0 means that model estimates rate from s4U fed data. If pnew is provided by user, must be  a vector
of length == number of s4U fed samples. The 1st element corresponds to the s4U induced mutation rate estimate for the 1st replicate of the 1st
experimental condition; the 2nd element corresponds to the s4U induced mutation rate estimate for the 2nd replicate of the 1st experimental condition,
etc.}

\item{pold}{Unlabeled read mutation rate; default of 0 means that model estimates rate from no-s4U fed data}

\item{no_ctl}{Logical; if TRUE, then -s4U control is not used for background mutation rate estimation}

\item{read_cut}{Minimum number of reads for a given feature-sample combo to be used for mut rate estimates}

\item{features_cut}{Number of features to estimate sample specific mutation rate with}

\item{nbin}{Number of bins for mean-variance relationship estimation. If NULL, max of 10 or (number of logit(fn) estimates)/100 is used}

\item{prior_weight}{Determines extent to which logit(fn) variance is regularized to the mean-variance regression line}

\item{MLE}{Logical; if TRUE then replicate logit(fn) is estimated using maximum likelihood; if FALSE more conservative Bayesian hypothesis testing is used}

\item{lower}{Lower bound for MLE with L-BFGS-B algorithm}

\item{upper}{Upper bound for MLE with L-BFGS-B algorithm}

\item{se_max}{Uncertainty given to those transcripts with estimates at the upper or lower bound sets. This prevents downstream errors due to
abnormally high standard errors due to transcripts with extreme kinetics}

\item{mut_reg}{If MLE has instabilities, empircal mut rate will be used to estimate fn, multiplying pnew by 1+mut_reg and pold by 1-mut_reg to regularize fn}

\item{p_mean}{Mean of normal distribution used as prior penalty in MLE of logit(fn)}

\item{p_sd}{Standard deviation of normal distribution used as prior peanlty in MLE of logit(fn)}

\item{StanRate}{Logical; if TRUE, a simple Stan model is used to estimate mutation rates for fast_analysis; this may add a couple minutes
to the runtime of the analysis.}

\item{Stan_data}{List; if StanRate is TRUE, then this is the data passed to the Stan model to estimate mutation rates. If using the \code{DynamicSeqFit}
wrapper of \code{fast_analysis}, then this is created automatically.}
}
\value{
List with dataframes providing information about replicate-specific and pooled analysis results. The output includes:
\itemize{
\item Fn_Estimates; dataframe with estimates for the fraction new and fraction new uncertainty for each feature in each replicate.
The columns of this dataframe are:
\itemize{
\item Feature_ID; Numerical ID of feature
\item Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
\item Replicate; Numerical ID for replicate
\item logit_fn; logit(fraction new) estimate, unregularized
\item logit_fn_se; logit(fraction new) uncertainty, unregularized and obtained from Fisher Information
\item nreads; Number of reads mapping to the feature in the sample for which the estimates were obtained
\item log_kdeg; log of degradation rate constant (kdeg) estimate, ungregularized
\item kdeg; degradation rate constant (kdeg) estimate
\item log_kd_se; log(kdeg) uncertainty, unregularized and obtained from Fisher Information
\item sample; Sample name
\item XF; Original feature name
}
\item Regularized_ests; dataframe with average fraction new and kdeg estimates, averaged across the replicates and regularized
using priors informed by the entire dataset. The columns of this dataframe are:
\itemize{
\item Feature_ID; Numerical ID of feature
\item Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
\item avg_log_kdeg; Weighted average of log(kdeg) from each replicate, weighted by sample and feature-specific read depth
\item sd_log_kdeg; Standard deviation of the log(kdeg) estimates
\item nreads; Total number of reads mapping to the feature in that condition
\item sdp; Prior standard deviation for fraction new estimate regularization
\item theta_o; Prior mean for fraction new estimate regularization
\item sd_post; Posterior uncertainty
\item log_kdeg_post; Posterior mean for log(kdeg) estimate
\item kdeg; exp(log_kdeg_post)
\item kdeg_sd; kdeg uncertainty
\item XF; Original feature name
}
\item Effects_df; dataframe with estimates of the effect size (change in logit(fn)) comparing each experimental condition to the
reference sample for each feature. This dataframe also includes p-values obtained from a moderated t-test. The columns of this
dataframe are:
\itemize{
\item Feature_ID; Numerical ID of feature
\item Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
\item L2FC_kdeg; Log2 fold change (L2FC) kdeg estimate
\item effect; L2FC_kdeg (repeated because it is used elsewhere and used to be difference in logit(fn)s)
\item se; Uncertainty in L2FC_kdeg
\item pval; P-value obtained using effect_size, se, and a moderated t-test
\item padj; pval adjusted for multiple testing using Benjamini-Hochberg procedure
\item XF; Original feature name
}
\item Mut_rates; list of two elements. The 1st element is a dataframe of s4U induced mutation rate estimates, where the mut column
represents the experimental ID and the rep column represents the replicate ID. The 2nd element is the single background mutation
rate estimate used
\item Hyper_Parameters; vector of two elements, named a and b. These are the hyperparameters estimated from the uncertainties for each
feature, and represent the two parameters of a Scaled Inverse Chi-Square distribution. Importantly, a is the number of additional
degrees of freedom provided by the sharing of uncertainty information across the dataset, to be used in the moderated t-test.
\item Mean_Variance_lms; linear model objects obtained from the uncertainty vs. read count regression model. One model is run for each Exp_ID
}
}
\description{
\code{fast_analysis} analyzes nucleotide recoding data using either maximum likelihood estimation with the L-BFGS-B algorithm
implemented by \code{stats::optim} or a more conservative Bayesian hypothesis testing strategy. Output includes fraction new
estimates for individual estimates as well as fraction news and effect sizes (L2FC(kdeg)s and changes in logit(fraction new))
averaged across replicate data. Averaging takes into account uncertainties estimated using the Fisher Information and estimates
are regularized using analytical results of fully Bayesian models. The result is that fraction news are shrunk towards population means
and that uncertainties are shrunk towards a mean-variance trend estimated as part of the analysis.
}
\details{
Unless the user supplies estimates for pnew and pold, the first step of \code{fast_analysis} is to estimate the background
and s4U induced mutation rates. The former is best performed with a -s4U control sample, that is, a normal RNA-seq sample
that lacks a -s4U feed or TimeLapse chemistry conversion of s4U to a C analog. If this sample is missing, both background and
s4U induced mutation rates are estimated from the s4U fed samples. For the s4U mutation rate, features with sufficient read depth,
as defined by the \code{read_cut} parameter, and the highest mutation rates are assumed to be completely labeled. Thus, the
average mutation rates in these features is taken as the estimate of the s4U induced mutation rate in that sample. s4U induced mutation
rates are estimated on a per-sample basis as there is often much more variability in these mutation rates than in the background
mutation rates.

If a -s4U control is included, the background mutation rate is estimated using all features in the control sample(s) with read depths
greater than \code{read_cut}. The average mutation rate among these features is taken as the estimated background mutation rate,
and that background is assumed to be constant for all samples. If a -s4U control is missing, then a strategy similar to that used
to estimate s4U induced mutation rates is used. In this case, the lowest mutation rate features with sufficient read depths are used,
and there average mutation rate is the background mutation rate estimate, as these features are assumed to be almost entirely unlabeled.

Once mutation rates are estimated, fraction news for each feature in each sample are estimated. The default approach utilized is MLE
using the L-BFGS-B algorithm implemented in \code{stats::optim}. The assumed likelihood function is derived from a Poisson mixture
model with rates adjusted according to each feature's empirical U-content (the average number of Us present in sequencing reads mapping
to that feature in a particular sample). An alternative model is a more conservative Bayesian hypothesis testing model with an uninformative
labeled vs. unlabeled prior. The Bayesian hypothesis testing model estimates the posterior probability that a set of reads with equivalent mutation
and U content are labeled using a binomial likelihood and a prior of 1/2 labeled and 1/2 unlabeled. For example, if the background
mutation rate is 0.001 and the s4U induced mutation rate is 0.1, then reads with 1 mutation in 25 Us have a posterior probability of being
labeled of 0.891. If there are 15 such reads, then 15 x 0.891 (or 13.365) of those reads are called labeled and 15 x (1 - 0.891)
(or 1.635) are called unlabeled. The estimated fraction new is then the number of reads called labeled divided by the total number
of reads. This approach biases fraction new estimates towards 0.5 and is thus a more conservative estimate of the fraction new
in each sample, as it is very skeptical of extreme fraction new values. This approach avoids some of the numerical instabilities
of MLE strategies and can thus be favorable if running into problems performing the MLE fit.

Once fraction news are estimated, the uncertainty in the fraction new is estimated using the Fisher Information. In the limit of
large datasets, the variance of the MLE is inversely related to the Fisher Information evaluated at the MLE. Mixture models are
typically singular, meaning that the Fisher information matrix is not positive definite and asymptotic results for the variance
do not necessarily hold. This can be understood as arising due to nothing stopping the model from estimating a background mutation
rate of 0. As the mutation rates are estimated a priori and fixed to be > 0, these problems are eliminated. In addition, when assessing
the uncertainty of replicate fraction new estimates, the size of the dataset is the raw number of sequencing reads that map to a
particular feature. This number is often large (>100) which increases the validity of invoking asymptotics.

With fraction news and their uncertainties estimated, replicate estimates are pooled and regularized. There are two key steps in this
downstream analysis. 1st, the uncertainty for each feature is used to extrapolate a linear log(uncertainty) vs. log10(read depth) trend,
and uncertainties for individual features are shrunk towards the regression line. The uncertainty for each feature is a combination of the
Fisher Information asymptotic uncertainty as well as the amount of variability seen between estimates. Regularization of uncertainty
estimates is performed using the analytic results of a Normal distribution likelihood with known mean and unknown variance and conjugate
priors. The prior parameters are estimated from the regression and amount of variability about the regression line. The strength of
regularization can be tuned by adjusting the \code{prior_weight} parameter, with larger numbers yielding stronger shrinkage towards
the regression line. The 2nd step is to regularize the average fraction new estimates. This is done using the analytic results of a
Normal distribution likelihood model with unknown mean and known variance and conjugate priors. The prior parameters are estimated from the
population wide fraction new distribution (using its mean and standard deviation as the mean and standard deviation of the normal prior).
In the 1st step, the known mean is assumed to be the average fraction new, averaged across replicates and weighted by the number of reads
mapping to the feature in each replicate. In the 2nd step, the known variance is assumed to be that obtained following regularization
of the uncertainty estimates

Effect sizes (changes in fraction new) are obtained as the difference in logit(fraction new) means between the reference and experimental
sample(s), and the logit(fraction new)s are assumed to be independent so that the variance of the effect size is the sum of the
logit(fraction new) variances. P-values assessing the significance of the effect size are obtained using a moderated t-test with number
of degrees of freedom determined from the uncertainty regression hyperparameters and are adjusted for multiple testing using the Benjamini-
Hochberg procedure to control false discovery rates (FDRs).
}
