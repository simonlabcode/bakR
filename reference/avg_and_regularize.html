<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Efficiently average replicates of nucleotide recoding data and regularize — avg_and_regularize • bakR</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Efficiently average replicates of nucleotide recoding data and regularize — avg_and_regularize"><meta name="description" content="avg_and_regularize pools and regularizes replicate estimates of kinetic parameters. There are two key steps in this
downstream analysis. 1st, the uncertainty for each feature is used to fit a linear ln(uncertainty) vs. log10(read depth) trend,
and uncertainties for individual features are shrunk towards the regression line. The uncertainty for each feature is a combination of the
Fisher Information asymptotic uncertainty as well as the amount of variability seen between estimates. Regularization of uncertainty
estimates is performed using the analytic results of a Normal distribution likelihood with known mean and unknown variance and conjugate
priors. The prior parameters are estimated from the regression and amount of variability about the regression line. The strength of
regularization can be tuned by adjusting the prior_weight parameter, with larger numbers yielding stronger shrinkage towards
the regression line. The 2nd step is to regularize the average kdeg estimates. This is done using the analytic results of a
Normal distribution likelihood model with unknown mean and known variance and conjugate priors. The prior parameters are estimated from the
population wide kdeg distribution (using its mean and standard deviation as the mean and standard deviation of the normal prior).
In the 1st step, the known mean is assumed to be the average kdeg, averaged across replicates and weighted by the number of reads
mapping to the feature in each replicate. In the 2nd step, the known variance is assumed to be that obtained following regularization
of the uncertainty estimates."><meta property="og:description" content="avg_and_regularize pools and regularizes replicate estimates of kinetic parameters. There are two key steps in this
downstream analysis. 1st, the uncertainty for each feature is used to fit a linear ln(uncertainty) vs. log10(read depth) trend,
and uncertainties for individual features are shrunk towards the regression line. The uncertainty for each feature is a combination of the
Fisher Information asymptotic uncertainty as well as the amount of variability seen between estimates. Regularization of uncertainty
estimates is performed using the analytic results of a Normal distribution likelihood with known mean and unknown variance and conjugate
priors. The prior parameters are estimated from the regression and amount of variability about the regression line. The strength of
regularization can be tuned by adjusting the prior_weight parameter, with larger numbers yielding stronger shrinkage towards
the regression line. The 2nd step is to regularize the average kdeg estimates. This is done using the analytic results of a
Normal distribution likelihood model with unknown mean and known variance and conjugate priors. The prior parameters are estimated from the
population wide kdeg distribution (using its mean and standard deviation as the mean and standard deviation of the normal prior).
In the 1st step, the known mean is assumed to be the average kdeg, averaged across replicates and weighted by the number of reads
mapping to the feature in each replicate. In the 2nd step, the known variance is assumed to be that obtained following regularization
of the uncertainty estimates."><meta property="og:image" content="https://simonlabcode.github.io/bakR/logo.png"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">bakR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/Getting-Started.html">Differential kinetic analysis with bakR</a></li>
    <li><a class="dropdown-item" href="../articles/bakR-Quickstart.html">bakR for people in a hurry</a></li>
    <li><a class="dropdown-item" href="../articles/Differential-Synth.html">Differential synthesis analysis with bakR and DESeq2</a></li>
    <li><a class="dropdown-item" href="../articles/bakR-Fn.html">GRAND-SLAM output/fn estimates as bakR input</a></li>
    <li><a class="dropdown-item" href="../articles/Dropout.html">Correcting for dropout</a></li>
    <li><a class="dropdown-item" href="../articles/Troubleshooting.html">Troubleshooting analyses of NR-seq data with bakR</a></li>
    <li><a class="dropdown-item" href="../articles/NSS.html">Steady-state quasi-independent mechanistic investigations</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/simonlabcode/bakR/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Efficiently average replicates of nucleotide recoding data and regularize</h1>
      <small class="dont-index">Source: <a href="https://github.com/simonlabcode/bakR/blob/master/R/Fast_analysis.R" class="external-link"><code>R/Fast_analysis.R</code></a></small>
      <div class="d-none name"><code>avg_and_regularize.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><code>avg_and_regularize</code> pools and regularizes replicate estimates of kinetic parameters. There are two key steps in this
downstream analysis. 1st, the uncertainty for each feature is used to fit a linear ln(uncertainty) vs. log10(read depth) trend,
and uncertainties for individual features are shrunk towards the regression line. The uncertainty for each feature is a combination of the
Fisher Information asymptotic uncertainty as well as the amount of variability seen between estimates. Regularization of uncertainty
estimates is performed using the analytic results of a Normal distribution likelihood with known mean and unknown variance and conjugate
priors. The prior parameters are estimated from the regression and amount of variability about the regression line. The strength of
regularization can be tuned by adjusting the <code>prior_weight</code> parameter, with larger numbers yielding stronger shrinkage towards
the regression line. The 2nd step is to regularize the average kdeg estimates. This is done using the analytic results of a
Normal distribution likelihood model with unknown mean and known variance and conjugate priors. The prior parameters are estimated from the
population wide kdeg distribution (using its mean and standard deviation as the mean and standard deviation of the normal prior).
In the 1st step, the known mean is assumed to be the average kdeg, averaged across replicates and weighted by the number of reads
mapping to the feature in each replicate. In the 2nd step, the known variance is assumed to be that obtained following regularization
of the uncertainty estimates.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">avg_and_regularize</span><span class="op">(</span></span>
<span>  <span class="va">Mut_data_est</span>,</span>
<span>  <span class="va">nreps</span>,</span>
<span>  <span class="va">sample_lookup</span>,</span>
<span>  <span class="va">feature_lookup</span>,</span>
<span>  nbin <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  NSS <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  Chase <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  BDA_model <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  null_cutoff <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  Mutrates <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  ztest <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-mut-data-est">Mut_data_est<a class="anchor" aria-label="anchor" href="#arg-mut-data-est"></a></dt>
<dd><p>Dataframe with fraction new estimation information. Required columns are:</p><ul><li><p>fnum; numerical ID of feature</p></li>
<li><p>reps; numerical ID of replicate</p></li>
<li><p>mut; numerical ID of experimental condition (Exp_ID)</p></li>
<li><p>logit_fn_rep; logit(fn) estimate</p></li>
<li><p>kd_rep_est; kdeg estimate</p></li>
<li><p>log_kd_rep_est; log(kdeg) estimate</p></li>
<li><p>logit_fn_se; logit(fn) estimate uncertainty</p></li>
<li><p>log_kd_se; log(kdeg) estimate uncertainty</p></li>
</ul></dd>


<dt id="arg-nreps">nreps<a class="anchor" aria-label="anchor" href="#arg-nreps"></a></dt>
<dd><p>Vector of number of replicates in each experimental condition</p></dd>


<dt id="arg-sample-lookup">sample_lookup<a class="anchor" aria-label="anchor" href="#arg-sample-lookup"></a></dt>
<dd><p>Dictionary mapping sample names to various experimental details</p></dd>


<dt id="arg-feature-lookup">feature_lookup<a class="anchor" aria-label="anchor" href="#arg-feature-lookup"></a></dt>
<dd><p>Dictionary mapping feature IDs to original feature names</p></dd>


<dt id="arg-nbin">nbin<a class="anchor" aria-label="anchor" href="#arg-nbin"></a></dt>
<dd><p>Number of bins for mean-variance relationship estimation. If NULL, max of 10 or (number of logit(fn) estimates)/100 is used</p></dd>


<dt id="arg-nss">NSS<a class="anchor" aria-label="anchor" href="#arg-nss"></a></dt>
<dd><p>Logical; if TRUE, logit(fn)s are compared rather than log(kdeg) so as to avoid steady-state assumption.</p></dd>


<dt id="arg-chase">Chase<a class="anchor" aria-label="anchor" href="#arg-chase"></a></dt>
<dd><p>Logical; Set to TRUE if analyzing a pulse-chase experiment. If TRUE, kdeg = -ln(fn)/tl where fn is the fraction of
reads that are s4U (more properly referred to as the fraction old in the context of a pulse-chase experiment)</p></dd>


<dt id="arg-bda-model">BDA_model<a class="anchor" aria-label="anchor" href="#arg-bda-model"></a></dt>
<dd><p>Logical; if TRUE, variance is regularized with scaled inverse chi-squared model. Otherwise a log-normal
model is used.</p></dd>


<dt id="arg-null-cutoff">null_cutoff<a class="anchor" aria-label="anchor" href="#arg-null-cutoff"></a></dt>
<dd><p>bakR will test the null hypothesis of |effect size| &lt; |null_cutoff|</p></dd>


<dt id="arg-mutrates">Mutrates<a class="anchor" aria-label="anchor" href="#arg-mutrates"></a></dt>
<dd><p>List containing new and old mutation rate estimates</p></dd>


<dt id="arg-ztest">ztest<a class="anchor" aria-label="anchor" href="#arg-ztest"></a></dt>
<dd><p>TRUE; if TRUE, then a z-test is used for p-value calculation rather than the more conservative moderated t-test.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>List with dataframes providing information about replicate-specific and pooled analysis results. The output includes:</p><ul><li><p>Fn_Estimates; dataframe with estimates for the fraction new and fraction new uncertainty for each feature in each replicate.
The columns of this dataframe are:</p><ul><li><p>Feature_ID; Numerical ID of feature</p></li>
<li><p>Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)</p></li>
<li><p>Replicate; Numerical ID for replicate</p></li>
<li><p>logit_fn; logit(fraction new) estimate, unregularized</p></li>
<li><p>logit_fn_se; logit(fraction new) uncertainty, unregularized and obtained from Fisher Information</p></li>
<li><p>nreads; Number of reads mapping to the feature in the sample for which the estimates were obtained</p></li>
<li><p>log_kdeg; log of degradation rate constant (kdeg) estimate, unregularized</p></li>
<li><p>kdeg; degradation rate constant (kdeg) estimate</p></li>
<li><p>log_kd_se; log(kdeg) uncertainty, unregularized and obtained from Fisher Information</p></li>
<li><p>sample; Sample name</p></li>
<li><p>XF; Original feature name</p></li>
</ul></li>
<li><p>Regularized_ests; dataframe with average fraction new and kdeg estimates, averaged across the replicates and regularized
using priors informed by the entire dataset. The columns of this dataframe are:</p><ul><li><p>Feature_ID; Numerical ID of feature</p></li>
<li><p>Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)</p></li>
<li><p>avg_log_kdeg; Weighted average of log(kdeg) from each replicate, weighted by sample and feature-specific read depth</p></li>
<li><p>sd_log_kdeg; Standard deviation of the log(kdeg) estimates</p></li>
<li><p>nreads; Total number of reads mapping to the feature in that condition</p></li>
<li><p>sdp; Prior standard deviation for fraction new estimate regularization</p></li>
<li><p>theta_o; Prior mean for fraction new estimate regularization</p></li>
<li><p>sd_post; Posterior uncertainty</p></li>
<li><p>log_kdeg_post; Posterior mean for log(kdeg) estimate</p></li>
<li><p>kdeg; exp(log_kdeg_post)</p></li>
<li><p>kdeg_sd; kdeg uncertainty</p></li>
<li><p>XF; Original feature name</p></li>
</ul></li>
<li><p>Effects_df; dataframe with estimates of the effect size (change in logit(fn)) comparing each experimental condition to the
reference sample for each feature. This dataframe also includes p-values obtained from a moderated t-test. The columns of this
dataframe are:</p><ul><li><p>Feature_ID; Numerical ID of feature</p></li>
<li><p>Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)</p></li>
<li><p>L2FC(kdeg); Log2 fold change (L2FC) kdeg estimate or change in logit(fn) if NSS TRUE</p></li>
<li><p>effect; LFC(kdeg)</p></li>
<li><p>se; Uncertainty in L2FC_kdeg</p></li>
<li><p>pval; P-value obtained using effect_size, se, and a z-test</p></li>
<li><p>padj; pval adjusted for multiple testing using Benjamini-Hochberg procedure</p></li>
<li><p>XF; Original feature name</p></li>
</ul></li>
<li><p>Mut_rates; list of two elements. The 1st element is a dataframe of s4U induced mutation rate estimates, where the mut column
represents the experimental ID and the rep column represents the replicate ID. The 2nd element is the single background mutation
rate estimate used</p></li>
<li><p>Hyper_Parameters; vector of two elements, named a and b. These are the hyperparameters estimated from the uncertainties for each
feature, and represent the two parameters of a Scaled Inverse Chi-Square distribution. Importantly, a is the number of additional
degrees of freedom provided by the sharing of uncertainty information across the dataset, to be used in the moderated t-test.</p></li>
<li><p>Mean_Variance_lms; linear model objects obtained from the uncertainty vs. read count regression model. One model is run for each Exp_ID</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>Effect sizes (changes in kdeg) are obtained as the difference in log(kdeg) means between the reference and experimental
sample(s), and the log(kdeg)s are assumed to be independent so that the variance of the effect size is the sum of the
log(kdeg) variances. P-values assessing the significance of the effect size are obtained using a moderated t-test with number
of degrees of freedom determined from the uncertainty regression hyperparameters and are adjusted for multiple testing using the Benjamini-
Hochberg procedure to control false discovery rates (FDRs).</p>
<p>In some cases, the assumed ODE model of RNA metabolism will not accurately model the dynamics of a biological system being analyzed.
In these cases, it is best to compare logit(fraction new)s directly rather than converting fraction new to log(kdeg).
This analysis strategy is implemented when <code>NSS</code> is set to TRUE. Comparing logit(fraction new) is only valid
If a single metabolic label time has been used for all samples. For example, if a label time of 1 hour was used for NR-seq
data from WT cells and a 2 hour label time was used in KO cells, this comparison is no longer valid as differences in
logit(fraction new) could stem from differences in kinetics or label times.</p>
    </div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Isaac Vock.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

