Mut_data <- df
Mut_data <- Mut_data[Mut_data$type == 1,]
ngene <- max(Mut_data$fnum)
num_conds <- max(Mut_data$mut)
nreps <- max(Mut_data$reps)
sample_lookup <- Mut_data[, c("sample", "mut", "reps")] %>% dplyr::distinct()
feature_lookup <- Mut_data[,c("fnum", "XF")] %>% dplyr::distinct()
# Estimate fraction new in each replicate using binomial model
message("Estimating fraction labeled")
Mut_data <- merge(Mut_data, New_data_estimate, by = c("mut", "reps"))
if(!MLE){
# Bayesian Hypothesis Testing Method
Mut_data_est <- Mut_data %>% dplyr::group_by(fnum, mut, reps, TC, nT) %>%
# mutate(avg_mut = TC/nT) %>%
# #mutate(prior_new = ifelse(avg_mut >= (pnew_est - 0.01), 0.99, (avg_mut + 0.01)/pnew_est )) %>%
# mutate(prior_new = 0.9)%>%
dplyr::mutate(New_prob = stats::dbinom(TC, size=nT, prob=pnew)) %>%
dplyr::mutate(Old_prob = stats::dbinom(TC, size = nT, prob = pold)) %>%
dplyr::mutate(News = n*(New_prob/(New_prob + Old_prob))) %>%
dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps) %>%
dplyr::summarise(nreads = sum(n), Fn_rep_est = sum(News)/nreads) %>%
dplyr::mutate(logit_fn_rep = ifelse(Fn_rep_est == 1, logit(0.999), ifelse(Fn_rep_est == 0, logit(0.001), logit(Fn_rep_est)))) %>%
dplyr::ungroup()
}else{
# MLE
mixed_lik <- function(lam_n, lam_o, TC, n, logit_fn){
logl <- sum(n*log(inv_logit(logit_fn)*(lam_n^TC)*exp(-lam_n) + (1-inv_logit(logit_fn))*(lam_o^TC)*exp(-lam_o) ))
return(-logl)
}
Mut_data_est <- Mut_data %>% dplyr::ungroup() %>% dplyr::mutate(lam_n = pnew*nT, lam_o = pold*nT) %>%
dplyr::group_by(fnum, mut, reps, TC) %>%
dplyr::summarise(lam_n = sum(lam_n*n)/sum(n), lam_o = sum(lam_o*n)/sum(n),
n = sum(n), .groups = "keep") %>%
dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps) %>%
dplyr::summarise(logit_fn_rep = optim(0.5, mixed_lik, TC = TC, n = n, lam_n = sum(lam_n*n)/sum(n), lam_o = sum(lam_o*n)/sum(n), method = "L-BFGS-B", lower = lower, upper = upper)$par, nreads =sum(n), .groups = "keep") %>%
dplyr::mutate(logit_fn_rep = ifelse(logit_fn_rep == lower, runif(1, lower-0.2, lower), ifelse(logit_fn_rep == upper, runif(1, upper, upper+0.2), logit_fn_rep))) %>%
dplyr::mutate(Fn_rep_est = inv_logit(logit_fn_rep)) %>%
dplyr::ungroup()
}
message("Estimating per replicate uncertainties")
Mut_data <- merge(Mut_data, Mut_data_est[, c("logit_fn_rep", "fnum", "mut", "reps")], by = c("fnum", "mut", "reps"))
## Estimate Fisher Info and uncertainties
## Could make more efficient by summarizing over nT info and using U_content to adjust pnew*avg_U
Mut_data <- Mut_data %>% dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps, TC, pnew, logit_fn_rep) %>%
dplyr::summarise(U_cont = sum(nT*n)/sum(n), n = sum(n), .groups = "keep") %>%
dplyr::mutate(Exp_l_fn = exp(logit_fn_rep)) %>%
# dplyr::mutate(fn = inv_logit(Mut_data_est$logit_fn_rep[(Mut_data_est$mut == mut) & (Mut_data_est$reps == reps) & (Mut_data_est$fnum == fnum)])) %>%
# dplyr::mutate(Fisher_fn_num = ((pnew_est*nT)^TC)*exp(-pnew_est*nT) - ((pold*nT)^TC)*exp(-pold*nT) ) %>%
# dplyr::mutate(Fisher_fn_den = fn*Fisher_fn_num + ((pold*nT)^TC)*exp(-pold*nT)) %>%
dplyr::mutate(Inv_Fisher_Logit_3 = 1/(((pnew/pold)^TC)*exp(-(U_cont)*(pnew - pold)) - 1 )) %>%
dplyr::mutate(Inv_Fisher_Logit_1 = 1 + Exp_l_fn ) %>%
dplyr::mutate(Inv_Fisher_Logit_2 = ((1 + Exp_l_fn)^2)/Exp_l_fn) %>%
dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps) %>%
dplyr::summarise(Fisher_Logit = sum(n/((Inv_Fisher_Logit_1 + Inv_Fisher_Logit_2*Inv_Fisher_Logit_3)^2))/sum(n), tot_n = sum(n)) %>% #,
#Fisher_fn = sum(n*((Fisher_fn_num/Fisher_fn_den)^2)), tot_n = sum(n)) %>%
dplyr::mutate(Logit_fn_se = 1/sqrt(tot_n*Fisher_Logit)) %>%
dplyr::mutate(Logit_fn_se = ifelse(Logit_fn_se > se_max, se_max, Logit_fn_se))#, Fn_se = 1/sqrt(tot_n*Fisher_fn))
Mut_data_est$logit_fn_se <- Mut_data$Logit_fn_se
if(is.null(nbin)){
nbin <- max(c(round(ngene*num_conds*nreps/100), 10))
}
message("Estimating read count-variance relationship")
find_reals <- function(x){
roots <- RConics::cubic(x)
return(roots[abs(roots) > 2 & (abs(roots) == roots)])
}
Binned_data <- Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_var_log = log(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) )) %>%
dplyr::ungroup() %>%
dplyr::mutate(bin_ID = as.numeric(Hmisc::cut2(nreads, g = nbin))) %>% dplyr::group_by(bin_ID) %>%
dplyr::summarise(avg_reads = mean(log10(nreads)), avg_var = mean(fn_var_log),
var_of_var = var(fn_var_log)) %>% dplyr::ungroup() %>%
dplyr::mutate(two_params = (8*(avg_var^4))/var_of_var)
Binned_data
avg_var <- -2.46
var_of_var <- 0.662
two_params <- 445
find_reals(c(1,-(4 + 2*(avg_var^4)/var_of_var), two_params, two_params))
RConics::cubic(c(1,-(4 + 2*(avg_var^4)/var_of_var), two_params, two_params))
find_reals <- function(x){
roots <- RConics::cubic(x)
return(min(roots[abs(roots) > 2 & (abs(roots) == roots)]))
}
find_reals(c(1,-(4 + 2*(avg_var^4)/var_of_var), two_params, two_params))
Binned_data <- Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_var_log = log(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) )) %>%
dplyr::ungroup() %>%
dplyr::mutate(bin_ID = as.numeric(Hmisc::cut2(nreads, g = nbin))) %>% dplyr::group_by(bin_ID) %>%
dplyr::summarise(avg_reads = mean(log10(nreads)), avg_var = mean(fn_var_log),
var_of_var = var(fn_var_log)) %>% dplyr::ungroup() %>%
dplyr::mutate(two_params = (8*(avg_var^4))/var_of_var) %>%
dplyr::mutate(a_hyper = find_reals(c(1, -(4 + 2*(avg_var^4)/var_of_var), two_params, two_params)) ) %>%
dplyr::mutate(b_hyper = (avg_var*(a_hyper - 2))/a_hyper )
Binned_data
-(4 + 2*(avg_var^4)/var_of_var)
Binned_data %>%
mutate(a_hyper = find_reals(c(1, -114.6401, 445, 445)))
Binned_data %>%
dplyr::mutate(a_hyper = find_reals(c(1, -114.6401, 445, 445)))
Binned_data %>%
dplyr::mutate(a_hyper = find_reals(c(1, -114.6401, two_params, 445)))
Binned_data <- Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_var_log = log(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) )) %>%
dplyr::ungroup() %>%
dplyr::mutate(bin_ID = as.numeric(Hmisc::cut2(nreads, g = nbin))) %>% dplyr::group_by(bin_ID) %>%
dplyr::summarise(avg_reads = mean(log10(nreads)), avg_var = mean(fn_var_log),
var_of_var = var(fn_var_log)) %>% dplyr::ungroup() %>%
dplyr::mutate(two_params = (8*(avg_var^4))/var_of_var) %>% dplyr::group_by(avg_var, var_of_var, two_params) %>%
dplyr::mutate(a_hyper = find_reals(c(1, -(4 + 2*(avg_var^4)/var_of_var), two_params, two_params)) ) %>% dplyr::ungroup() %>%
dplyr::mutate(b_hyper = (avg_var*(a_hyper - 2))/a_hyper )
Binned_data
Binned_data <- Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_var_log = log(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) )) %>%
dplyr::ungroup() %>%
dplyr::mutate(bin_ID = as.numeric(Hmisc::cut2(nreads, g = nbin))) %>% dplyr::group_by(bin_ID) %>%
dplyr::summarise(avg_reads = mean(log10(nreads)), avg_var = mean(fn_var_log),
var_of_var = var(fn_var_log)) %>% dplyr::ungroup() %>%
dplyr::mutate(two_params = (8*(avg_var^4))/var_of_var) %>% dplyr::group_by(avg_var, var_of_var, two_params) %>%
dplyr::mutate(a_hyper = find_reals(c(1, -(4 + 2*(avg_var^4)/var_of_var), two_params, two_params)) ) %>% dplyr::ungroup() %>%
dplyr::group_by(avg_var, a_hyper) %>%
dplyr::mutate(b_hyper = (avg_var*(a_hyper - 2))/a_hyper )
Binned_data
Binned_data <- Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_var_log = log(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) )) %>%
dplyr::ungroup() %>%
dplyr::mutate(bin_ID = as.numeric(Hmisc::cut2(nreads, g = nbin))) %>% dplyr::group_by(bin_ID) %>%
dplyr::summarise(avg_reads = mean(log10(nreads)), avg_var = mean(exp(fn_var_log)),
var_of_var = var(exp(fn_var_log))) %>% dplyr::ungroup() %>%
dplyr::mutate(two_params = (8*(avg_var^4))/var_of_var) %>% dplyr::group_by(avg_var, var_of_var, two_params) %>%
dplyr::mutate(a_hyper = find_reals(c(1, -(4 + 2*(avg_var^4)/var_of_var), two_params, two_params)) ) %>% dplyr::ungroup() %>%
dplyr::group_by(avg_var, a_hyper) %>%
dplyr::mutate(b_hyper = (avg_var*(a_hyper - 2))/a_hyper )
Binned_data
hist(Binned_data$a_hyper)
hist(Binned_data$avg_reads)
devtools::load_all()
Binned_data <- Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_var_log = log(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) )) %>%
dplyr::ungroup() %>%
dplyr::mutate(bin_ID = as.numeric(Hmisc::cut2(nreads, g = nbin))) %>% dplyr::group_by(bin_ID) %>%
dplyr::summarise(avg_reads = mean(log10(nreads)), avg_var = mean(exp(fn_var_log)),
var_of_var = var(exp(fn_var_log))) %>% dplyr::ungroup() %>%
dplyr::mutate(two_params = (8*(avg_var^4))/var_of_var) %>% dplyr::group_by(avg_var, var_of_var, two_params) %>%
dplyr::mutate(a_hyper = find_reals(c(1, -(4 + 2*(avg_var^4)/var_of_var), two_params, two_params)) ) %>% dplyr::ungroup() %>%
dplyr::group_by(avg_var, a_hyper) %>%
dplyr::mutate(b_hyper = (avg_var*(a_hyper - 2))/a_hyper )
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
max(sim_list$sim_list$Fn_rep_sim$fn)
min(sim_list$sim_list$Fn_rep_sim$fn)
0.977*0.05 + (1-0.977)*0.001
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
Binned_data$a_hyper[Binned_data$bin_ID == 1]
Binned_data$a_hyper[Binned_data$bin_ID == 2]
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
test_df <- data.frame(a = 1:10, bin_ID = 1:10)
test_df
test_df %>% mutate(a_hyper = Binned_data$a_hyper[Binned_data$bin_ID == bin_ID])
test_df %>% dplyr::mutate(a_hyper = Binned_data$a_hyper[Binned_data$bin_ID == bin_ID])
test_df %>% dplyr::group_by(bin_ID) %>% dplyr::mutate(a_hyper = Binned_data$a_hyper[Binned_data$bin_ID == bin_ID])
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
warnings()
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
warnings()
hist(Binned_data$b_hyper)
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
Fit$Fast_Fit$Hyper_Parameters
1/(4*0.0235)
plotVolcano(Fit$Fast_Fit)
Fit$Fast_Fit$Effects_df$se
Fit$Fast_Fit$Regularized_ests$sd_post
hist(Fit$Fast_Fit$Regularized_ests$sd_post)
hist(Fit$Fast_Fit$Regularized_ests$sd_logit_fn)
vector("list", length = 2)
test <- vector("list", length = 2)
test[[1]] <- c(1, 20)
test[[1]][1]
Fit$Fast_Fit$Mean_Variance_lm
stats::residuals(Fit$Fast_Fit$Mean_Variance_lm)
hist(stats::residuals(Fit$Fast_Fit$Mean_Variance_lm))
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
plotVolcano(Fit$Fast_Fit)
sum(Fit$Fast_Fit$Effects_df$padj < 0)
sum(Fit$Fast_Fit$Effects_df$padj < 0.05)
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] <= 1000)/sum(Fit$Fast_Fit$Effects_df$padj < 0.05)
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] <= 500)/sum(Fit$Fast_Fit$Effects_df$padj < 0.05)
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] <= 500)/sum(Fit$Fast_Fit$Effects_df$padj < 0.05)
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] > 500)/500
sim_list <- sim_DynamicSeqData(2000, num_kd_DE = c(0, 200),num_conds = 2, p_new = 0.05,
nreps = 3, eff_sd = 0.7, tl = 60, fn_sd = 1)
DynData <- sim_list$DynData
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
# FDR
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] <= 500)/sum(Fit$Fast_Fit$Effects_df$padj < 0.05)
# Power
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] > 500)/500
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
# FDR
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] <= 500)/sum(Fit$Fast_Fit$Effects_df$padj < 0.05)
# Power
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] > 500)/500
sim_list <- sim_DynamicSeqData(2000, num_kd_DE = c(0, 200),num_conds = 2, p_new = 0.05,
nreps = 3, eff_sd = 0.4, tl = 60, fn_sd = 1)
DynData <- sim_list$DynData
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
# FDR
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] <= 500)/sum(Fit$Fast_Fit$Effects_df$padj < 0.05)
# Power
sum(Fit$Fast_Fit$Effects_df$XF[Fit$Fast_Fit$Effects_df$padj < 0.05] > 500)/500
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
Fit$Fast_Fit$Mean_Variance_lms[[1]]
Fit$Fast_Fit$Mean_Variance_lms[[2]]
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
devtools::load_all()
devtools::load_all()
devtools::load_all()
Fit <- DynamicSeqFit(DynData, lower = -9, upper = 9)
Fit <- DynamicSeqFit(Fit, lower = -9, upper = 9, FastRerun = TRUE)
devtools::load_all()
Fit <- DynamicSeqFit(Fit, lower = -9, upper = 9, FastRerun = TRUE)
devtools::load_all()
Fit <- DynamicSeqFit(Fit, lower = -9, upper = 9, FastRerun = TRUE)
Reg_all <- Fit$Fast_Fit$Regularized_ests
plot(Reg_all$sd_logit_fn, Reg_all$sd_post)
Reg_all$sd_post
df <- Fit$Data_lists$Fast_df
pnew <- rep(0.05, times = 6)
pold <- 0.002
no_ctl <- FALSE
read_cut <- 50
features_cut <- 10
nbin <- NULL
MLE <- TRUE
lower <- -9
upper <- 9
se_max <- 2.5
nMT <- max(df$mut)
nreps <- max(df$reps)
logit <- function(x) log(x/(1-x))
inv_logit <- function(x) exp(x)/(1+exp(x))
#Trim df and name columns
if(is.null(pnew)){
message("Estimating labeled mutation rate")
Mut_data <- df
##New Mutation rate Estimation
# Extract only s4U labeled data to estimate s4U mut rate
New_data <- Mut_data[Mut_data$type == 1, ]
# Calculate avg. mut rate at each row
New_data$avg_mut <- New_data$TC/New_data$nT
# Remove rows with NAs
New_data <- New_data[!is.na(New_data$avg_mut),]
# calculate total number of mutations
# which is the avg. for that row of dataframe times n
# the number of reads that had the identical average
New_data$weight_mut <- New_data$avg_mut*New_data$n
# This is to estimate the total mutation rate for each gene in
# each replicate and each experimental condition
New_data_summary <- New_data %>%
dplyr::group_by(reps, mut, fnum) %>% # group by gene, replicate ID, and experiment ID
dplyr::summarise(avg_mut = sum(weight_mut)/sum(n), n = sum(n))
# Order datalist so that it's ordered by sample and then avg mutation rate
# Goal is to use the highest avg. mutation rates to estimate s4U mutation rate,
# assuming that highest mutation rates are from fast turnover, compeltely
# labeled transcripts
New_data_ordered <- New_data_summary[order(New_data_summary$mut, New_data_summary$reps, New_data_summary$avg_mut, decreasing=TRUE), ]
## This part has some magic numbers I should get rid of
## or move to user input
New_data_cutoff <- New_data_ordered[New_data_ordered$n > read_cut,]
# Check to make sure that the number of features that made it past the
# read count filter is still more than the total number of features required for
# mutation rate estimate
check <- New_data_cutoff %>% dplyr::ungroup() %>%
dplyr::count(mut, reps, sort = TRUE)
if(sum(check$n < features_cut) > 0){
stop("Not enough features made it past the read cutoff filter in one sample; try decreasing read_cut or features_cut")
}else{
New_data_estimate <- New_data_cutoff %>% dplyr::group_by(mut, reps) %>%
dplyr::summarise(pnew = mean(avg_mut[1:features_cut]))
message(paste0(c("Estimated pnews for each sample are:", capture.output(New_data_estimate)), collapse = "\n"))
}
}else{ # Need to construct pmut dataframe from User input
nMT <- max(df$mut)
nreps <- max(df$reps)
if(length(pnew) == 1){
pnew_vect <- rep(pnew, times = (nMT*nreps))
rep_vect <- rep(seq(from = 1, to = nreps), times = nMT)
mut_vect <- rep(seq(from = 1, to = nMT), each = nreps)
New_data_estimate <- data.frame(mut_vect, rep_vect, pnew_vect)
colnames(New_data_estimate) <- c("mut", "reps", "pnew")
} else if( length(pnew) != (nMT*nreps) ){
stop("User inputted pnew is not of length 1 or of length equal to number of samples")
} else{
rep_vect <- rep(seq(from = 1, to = nreps), times = nMT)
mut_vect <- rep(seq(from = 1, to = nMT), each = nreps)
New_data_estimate <- data.frame(mut_vect, rep_vect, pnew)
colnames(New_data_estimate) <- c("mut", "reps", "pnew")
}
}
if(is.null(pold)){
if((sum(df$type == 0) == 0) | (no_ctl)){ # Estimate using low mutation rate features
message("Estimating unlabeled mutation rate")
New_data <- df
# Calculate avg. mut rate at each row
New_data$avg_mut <- New_data$TC/New_data$nT
# Remove rows with NAs
New_data <- New_data[!is.na(New_data$avg_mut),]
# calculate total number of mutations
# which is the avg. for that row of dataframe times n
# the number of reads that had the identical average
New_data$weight_mut <- New_data$avg_mut*New_data$n
# This is to estimate the total mutation rate for each gene in
# each replicate and each experimental condition
New_data_summary <- New_data %>%
dplyr::group_by(fnum) %>% # group by gene, replicate ID, and experiment ID
dplyr::summarise(avg_mut = sum(weight_mut)/sum(n), n = sum(n), .groups = "keep")
# Order datalist so that it's ordered by sample and then avg mutation rate
# Goal is to use the lowest avg. mutation rates to estimate s4U mutation rate,
# assuming that highest mutation rates are from fast turnover, completely
# labeled transcripts
New_data_ordered <- New_data_summary[order(New_data_summary$avg_mut, decreasing=FALSE), ]
# Filter out for high read depth features
New_data_cutoff <- New_data_ordered[New_data_ordered$n > read_cut,]
# Check to make sure that the number of features that made it past the
# read count filter is still more than the total number of features required for
# mutation rate estimate
check <- nrow(New_data_cutoff)
if(check < features_cut){
stop("Not enough features made it past the read cutoff filter in one sample; try decreasing read_cut or features_cut")
}else{
pold <- stats::weighted.mean(New_data_cutoff$avg_mut[1:features_cut], w = New_data_cutoff$n[1:features_cut])
message(paste(c("Estimated pold is: ", pold), collapse = " "))
}
}else{ # Estimate using -s4U data
message("Estimating unlabeled mutation rate")
#Old mutation rate estimation
Mut_data <- df
Old_data <- Mut_data[Mut_data$type == 0, ]
Old_data$avg_mut <- Old_data$TC/Old_data$nT
# Remove rows with NAs
Old_data <- Old_data[!is.na(Old_data$avg_mut),]
Old_data$weight_mut <- Old_data$avg_mut*Old_data$n
#Old_data$n <- rep(1, times=nrow(Old_data))
Old_data_summary <- Old_data %>%
dplyr::group_by(reps, mut, fnum) %>% # group by gene, replicate ID, and experiment ID
dplyr::summarise(avg_mut = sum(weight_mut)/sum(n), n = sum(n))
# Order data differently than for s4U mut rate estimation
# Difference is that every mutation is a background mutation in these samples
# So we just want the highest confidence estimation, meaning we should only
# order by read counts
Old_data_ordered <- Old_data_summary[order(Old_data_summary$n, decreasing=TRUE), ]
## This part has some magic numbers I should get rid of
## or move to user input
Old_data_cutoff <- Old_data_ordered[Old_data_ordered$n > read_cut,]
# Check to make sure that the number of features that made it past the
# read count filter is still more than the total number of features required for
# mutation rate estimate
check <- nrow(Old_data_cutoff)
if(check < features_cut){
stop("Not enough features made it past the read cutoff filter in one sample; try decreasing read_cut or features_cut")
}else{
pold <- stats::weighted.mean(Old_data_cutoff$avg_mut[1:features_cut], w = Old_data_cutoff$n[1:features_cut])
message(paste(c("Estimated pold is: ", pold), collapse = " "))
}
}
}
if(!all(New_data_estimate$pnew - pold > 0)){
stop("All pnew must be > pold; did you input an unusually large pold?")
}
pmuts_list <- list(New_data_estimate, pold)
Mut_data <- df
Mut_data <- Mut_data[Mut_data$type == 1,]
ngene <- max(Mut_data$fnum)
num_conds <- max(Mut_data$mut)
nreps <- max(Mut_data$reps)
sample_lookup <- Mut_data[, c("sample", "mut", "reps")] %>% dplyr::distinct()
feature_lookup <- Mut_data[,c("fnum", "XF")] %>% dplyr::distinct()
# Estimate fraction new in each replicate using binomial model
message("Estimating fraction labeled")
Mut_data <- merge(Mut_data, New_data_estimate, by = c("mut", "reps"))
if(!MLE){
# Bayesian Hypothesis Testing Method
Mut_data_est <- Mut_data %>% dplyr::group_by(fnum, mut, reps, TC, nT) %>%
# mutate(avg_mut = TC/nT) %>%
# #mutate(prior_new = ifelse(avg_mut >= (pnew_est - 0.01), 0.99, (avg_mut + 0.01)/pnew_est )) %>%
# mutate(prior_new = 0.9)%>%
dplyr::mutate(New_prob = stats::dbinom(TC, size=nT, prob=pnew)) %>%
dplyr::mutate(Old_prob = stats::dbinom(TC, size = nT, prob = pold)) %>%
dplyr::mutate(News = n*(New_prob/(New_prob + Old_prob))) %>%
dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps) %>%
dplyr::summarise(nreads = sum(n), Fn_rep_est = sum(News)/nreads) %>%
dplyr::mutate(logit_fn_rep = ifelse(Fn_rep_est == 1, logit(0.999), ifelse(Fn_rep_est == 0, logit(0.001), logit(Fn_rep_est)))) %>%
dplyr::ungroup()
}else{
# MLE
mixed_lik <- function(lam_n, lam_o, TC, n, logit_fn){
logl <- sum(n*log(inv_logit(logit_fn)*(lam_n^TC)*exp(-lam_n) + (1-inv_logit(logit_fn))*(lam_o^TC)*exp(-lam_o) ))
return(-logl)
}
Mut_data_est <- Mut_data %>% dplyr::ungroup() %>% dplyr::mutate(lam_n = pnew*nT, lam_o = pold*nT) %>%
dplyr::group_by(fnum, mut, reps, TC) %>%
dplyr::summarise(lam_n = sum(lam_n*n)/sum(n), lam_o = sum(lam_o*n)/sum(n),
n = sum(n), .groups = "keep") %>%
dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps) %>%
dplyr::summarise(logit_fn_rep = optim(0.5, mixed_lik, TC = TC, n = n, lam_n = sum(lam_n*n)/sum(n), lam_o = sum(lam_o*n)/sum(n), method = "L-BFGS-B", lower = lower, upper = upper)$par, nreads =sum(n), .groups = "keep") %>%
dplyr::mutate(logit_fn_rep = ifelse(logit_fn_rep == lower, runif(1, lower-0.2, lower), ifelse(logit_fn_rep == upper, runif(1, upper, upper+0.2), logit_fn_rep))) %>%
dplyr::mutate(Fn_rep_est = inv_logit(logit_fn_rep)) %>%
dplyr::ungroup()
}
message("Estimating per replicate uncertainties")
Mut_data <- merge(Mut_data, Mut_data_est[, c("logit_fn_rep", "fnum", "mut", "reps")], by = c("fnum", "mut", "reps"))
## Estimate Fisher Info and uncertainties
## Could make more efficient by summarizing over nT info and using U_content to adjust pnew*avg_U
Mut_data <- Mut_data %>% dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps, TC, pnew, logit_fn_rep) %>%
dplyr::summarise(U_cont = sum(nT*n)/sum(n), n = sum(n), .groups = "keep") %>%
dplyr::mutate(Exp_l_fn = exp(logit_fn_rep)) %>%
# dplyr::mutate(fn = inv_logit(Mut_data_est$logit_fn_rep[(Mut_data_est$mut == mut) & (Mut_data_est$reps == reps) & (Mut_data_est$fnum == fnum)])) %>%
# dplyr::mutate(Fisher_fn_num = ((pnew_est*nT)^TC)*exp(-pnew_est*nT) - ((pold*nT)^TC)*exp(-pold*nT) ) %>%
# dplyr::mutate(Fisher_fn_den = fn*Fisher_fn_num + ((pold*nT)^TC)*exp(-pold*nT)) %>%
dplyr::mutate(Inv_Fisher_Logit_3 = 1/(((pnew/pold)^TC)*exp(-(U_cont)*(pnew - pold)) - 1 )) %>%
dplyr::mutate(Inv_Fisher_Logit_1 = 1 + Exp_l_fn ) %>%
dplyr::mutate(Inv_Fisher_Logit_2 = ((1 + Exp_l_fn)^2)/Exp_l_fn) %>%
dplyr::ungroup() %>%
dplyr::group_by(fnum, mut, reps) %>%
dplyr::summarise(Fisher_Logit = sum(n/((Inv_Fisher_Logit_1 + Inv_Fisher_Logit_2*Inv_Fisher_Logit_3)^2))/sum(n), tot_n = sum(n)) %>% #,
#Fisher_fn = sum(n*((Fisher_fn_num/Fisher_fn_den)^2)), tot_n = sum(n)) %>%
dplyr::mutate(Logit_fn_se = 1/sqrt(tot_n*Fisher_Logit)) %>%
dplyr::mutate(Logit_fn_se = ifelse(Logit_fn_se > se_max, se_max, Logit_fn_se))#, Fn_se = 1/sqrt(tot_n*Fisher_fn))
Mut_data_est$logit_fn_se <- Mut_data$Logit_fn_se
if(is.null(nbin)){
nbin <- max(c(round(ngene*num_conds*nreps/100), 10))
}
message("Estimating read count-variance relationship")
Binned_data <- Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>%
dplyr::mutate(bin_ID = as.numeric(Hmisc::cut2(nreads, g = nbin))) %>% dplyr::group_by(bin_ID, mut) %>%
dplyr::summarise(avg_reads = mean(log10(nreads)), avg_sd = mean(fn_sd_log))
## Regress avg_reads vs. avg_sd
lm_list <- vector("list", length = 2L)
lm_var <- lm_list
for(i in 1:nMT){
heterosked_lm <- stats::lm(avg_sd ~ avg_reads, data = Binned_data[Binned_data$mut == i,] )
h_int <- summary(heterosked_lm)$coefficients[1,1]
h_slope <- summary(heterosked_lm)$coefficients[2,1]
lm_list[[i]] <- c(h_int, h_slope)
lm_var[[i]] <- var(stats::residuals(heterosked_lm))
}
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(true_var = var(fn_sd_log - (lm_list[[mut]][1] + lm_list[[mut]][2]*log10(nreads) ) ))
true_vars$true_var
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) #%>%
true_vars
lm_list[[1]][1]
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_sd_log = var(log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) ) - (lm_list[[mut]][1] + lm_list[[mut]][2]*log10(nreads) ) ))
rlang::last_error()
rlang::last_trace()
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads),fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>% dplyr::group_by(mut) %>%
dplyr::summarise(true_var = var(fn_sd_log - (lm_list[[mut]][1] + lm_list[[mut]][2]*log10(nreads) ) ))
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads), fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>% dplyr::group_by(mut) %>%
dplyr::summarise(true_var = var(fn_sd_log - 2*log10(nreads) ) )
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads), fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>% dplyr::group_by(mut) %>% dplyr::mutate(slope = lm_list[[mut]][2], intercept = lm_list[[mut]][1]) %>%
dplyr::summarise(true_var = var(fn_sd_log - (intercept + slope*log10(nreads) ) ))
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads), fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>% dplyr::mutate(slope = lm_list[[mut]][2], intercept = lm_list[[mut]][1]) %>% dplyr::group_by(mut) %>%
dplyr::summarise(true_var = var(fn_sd_log - (intercept + slope*log10(nreads) ) ))
lm_list[[mut]][2]
lm_list[[1]][2]
lm_list[[1]][1]
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads), fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>% dplyr::group_by(mut) %>% dplyr::mutate(slope = lm_list[[mut]][2], intercept = lm_list[[mut]][1]) %>%
dplyr::summarise(true_var = var(fn_sd_log - (intercept + slope*log10(nreads) ) ))
true_vars <-  Mut_data_est %>% dplyr::group_by(fnum, mut) %>%
dplyr::summarise(nreads = sum(nreads), fn_sd_log = log(sqrt(1/sum(1/((sd(logit_fn_rep)^2) + logit_fn_se^2 ) ) ) )) %>%
dplyr::ungroup() %>% dplyr::group_by(fnum, mut) %>% dplyr::mutate(slope = lm_list[[mut]][2], intercept = lm_list[[mut]][1]) %>%
dplyr::group_by(mut) %>%
dplyr::summarise(true_var = var(fn_sd_log - (intercept + slope*log10(nreads) ) ))
true_vars
devtools::load_all()
Fit <- DynamicSeqFit(Fit, lower = -9, upper = 9, FastRerun = TRUE)
Reg_all <- Fit$Fast_Fit$Regularized_ests
plot(Reg_all$sd_logit_fn, Reg_all$sd_post)
